{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "# from pandas import DataFrame\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prep for looping over all pages.\n",
    "i=0\n",
    "url = \"https://www.ictergezocht.nl/it-vacatures/?pnr=\" + str(i)\n",
    "\n",
    "#Creating storage for URLS\n",
    "links1 = [] #Create open list to store links later. non spot\n",
    "links2 = [] #Create open list to store links later. spotlight vacancies\n",
    "links3 = [] #Both attempt <- Works!\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    i = i+1\n",
    "    if i == 75: #make sure it stops at last page. \n",
    "        break\n",
    "\n",
    "    #should stop the code in case of error\n",
    "    page = requests.get(url)\n",
    "    if page.status_code != 200:\n",
    "        break\n",
    "\n",
    "    url = \"https://www.ictergezocht.nl/it-vacatures/?pnr=\" + str(i)\n",
    "    print(url)\n",
    "    #Data retrieval prepetaion\n",
    "    uClient = uReq(url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "\n",
    "    page_soup1 = soup(page_html,\"html.parser\")\n",
    "\n",
    "    #Grabs each block (spot and non spot)\n",
    "    content_blocks = page_soup1.findAll(\"div\",{\"class\":\"content_block vacitem\"})\n",
    "\n",
    "    spot_content_blocks = page_soup1.findAll(\"div\",{\"class\":\"content_block vacitem spotlight \"})\n",
    "\n",
    "    #Grab specific info for each block\n",
    "    aaa = 0\n",
    "    for content_block in content_blocks:\n",
    "        aaa = aaa + 1\n",
    "        \n",
    "        t = content_block.find(\"a\",{\"class\":\"title\"})\n",
    "        try:\n",
    "            link = t.get('href')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        #title_block = content_block.findAll(\"a\",{\"class\":\"title\"})\n",
    "        #func_title = title_block[0].text\n",
    "\n",
    "        links1.append(link) #My attempt to save everything in the same list\n",
    "        links3.append(link) #Attempt all\n",
    "        print(aaa)\n",
    "        #print(\"Job title: \"+ func_title)\n",
    "        #print(link)\n",
    "\n",
    "\n",
    "    \n",
    "    for spot_content_block in spot_content_blocks:\n",
    "        aaa = aaa +1\n",
    "        t = spot_content_block.find(\"a\",{\"class\":\"title\"})\n",
    "        try:\n",
    "            link = t.get('href')\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        #spot_title_block = spot_content_block.findAll(\"a\",{\"class\":\"title\"})\n",
    "        #spot_func_title = spot_title_block[0].text\n",
    "\n",
    "        links2.append(link) #My attempt to save everything in the same list\n",
    "        links3.append(link) #Attempt all\n",
    "        print(aaa)\n",
    "        #print(\"Spotlight Job title: \"+ spot_func_title)\n",
    "        #print(link)\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "print(\"DONE stage1! Retrieving Links to Job descriptions\")\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(links1), len(links2), len(links3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Preparing CSV\n",
    "ict_old = pd.read_csv(\"ict-vacatures-text-all.csv\")\n",
    "filename = \"placeholder\"\n",
    "topics_dict = { \"Job_title\":[], \\\n",
    "                \"Company\":[], \\\n",
    "                \"Location\":[], \\\n",
    "                \"Offer\":[], \\\n",
    "               \"Link\":[], \\\n",
    "                \"Posted\":[], \\\n",
    "                \"Text\":[], \\\n",
    "                }\n",
    "\n",
    "\n",
    "urls = links3\n",
    "test_list = []\n",
    "for url in urls:\n",
    "    print(url)\n",
    "    uClient = uReq(url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "\n",
    "    vac_soup1 = soup(page_html,\"html.parser\")\n",
    "    data = vac_soup1.find(\"script\", type=\"application/ld+json\").text\n",
    "    data = ' '.join(re.sub(r\"(\\n|\\t)*\", \"\", data).split()) # --->>> REMOVE NEW LINE CHARS AND WHITESPACES\n",
    "\n",
    "    # --->>> PREPROCESSING DATA TO REPLACE HTML CODES WITH SIGNS\n",
    "    data = re.sub(r\"\\&amp;\", \"&\", data) # --->>> AMPERSAND '&'\n",
    "    data = re.sub(r\"\\&reg;\", \"\", data) # --->>> REGISTERED SIGN 'Â®'\n",
    "\n",
    "    # --->>> IMPORTANT <<<--- #\n",
    "    # --->>> APPARENTLY SOUP MAKES SOMETIMES A MISTAKE WITH WRITING ANOTHER COMMA AT THE END OF A BLOCK\n",
    "    # --->>> WHERE NO MORE BLOCKS ARE COMING AND JSON DOES NOT LIKE THAT...\n",
    "    data = re.sub(r\"\\},\\s\\}\", \"} }\", data) # --->>> REMOVE TRAILING COMMA IF NO MORE BLOCKS ARE COMING\n",
    "    \n",
    "    # --->>> NEW STEP <<<--- #\n",
    "    # --->>> USE A LOOKBEHIND AND A LOOKAHEAD AROUND A COMMA.\n",
    "    # --->>> IF THEY MATCH NUMBER COMMA NUMBER.\n",
    "    # --->>> REPLACE IT WITH NUMBER DOT NUMBER\n",
    "    # --->>> THERE ARE ALSO NO QUANTIFIERS POSSIBLE IN THE LOOKAHEAD/LOOKBEHIND\n",
    "    data = re.sub(r\"(?<=\\d),(?=\\d)\", \".\", data) # --->>> REPLACE A COMMA BETWEEN 2 NUMBERS WITH A DOT\n",
    "    \n",
    "    # --->>> END PREPROCESSING\n",
    "\n",
    "    # --->>> I USED THIS TO FIND THE ERRORS AT RUNTIME\n",
    "    try:\n",
    "        newDictionary = json.loads(data) # --->>> LOAD EVERYTHING TO A DICTIONARY\n",
    "    except (Exception, ArithmeticError) as e:\n",
    "        template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\" # --->>> ERROR MESSAGE TEMPLATE\n",
    "        message = template.format(type(e).__name__, e.args) # --->>> COMPILE ERROR MESSAGE\n",
    "        print (message) # --->>> PRINT THE ERROR\n",
    "        print(data) # --->>> PRINT DATA TO CHECK WHAT COULD CAUSE ERROR\n",
    "        sys.exit(\"JSONDecodeError\") # --->>> STOP EXECUTION OF SCRIPT\n",
    "\n",
    "    # --->>> THE VALUES WE ARE INTERESTED IN\n",
    "    # --->>> JUST ADD MORE IF YOU NEED\n",
    "    # --->>> USE print(newDictionary) TO FIND THE INDICES YOU ARE INTERESTED IN\n",
    "    # print(newDictionary)\n",
    "    title = newDictionary['title']\n",
    "    company = newDictionary['hiringOrganization']['name']\n",
    "    offer = newDictionary['@type']\n",
    "    location = newDictionary['jobLocation']['address']['addressLocality']\n",
    "    date = newDictionary['datePosted']\n",
    "\n",
    "    #Preparing for CSV\n",
    "    title.replace(\",\",\";\")\n",
    "    company.replace(\",\",\";\")\n",
    "    offer.replace(\",\",\";\")\n",
    "    location.replace(\",\",\";\")\n",
    "    date.replace(\",\",\";\")    \n",
    "    #Writing to Pandas frame\n",
    "    topics_dict[\"Job_title\"].append(title)\n",
    "    topics_dict[\"Company\"].append(company)\n",
    "    topics_dict[\"Location\"].append(location)\n",
    "    topics_dict[\"Offer\"].append(offer)\n",
    "    topics_dict[\"Link\"].append(url)\n",
    "    topics_dict[\"Posted\"].append(date)\n",
    "\n",
    "    all_text = []\n",
    "    vactexts = vac_soup1.findAll(\"div\",{\"class\":\"vactext\"}) \n",
    "    for vactext in vactexts:\n",
    "        tekst = vactext.text\n",
    "        tekst = re.sub(r\"[\\n\\t]*\", \"\", tekst)\n",
    "\n",
    "        all_text.append(tekst)\n",
    "\n",
    "        str_text = ''.join(all_text) \n",
    "        str_text = re.sub(r'[\\n\\t]',' ', str_text)\n",
    "    # --->>> END PREPROCESSING\n",
    "\n",
    "    # --->>> WRITE VALUES TO FILE\n",
    "    str_text.replace(\",\",\";\")\n",
    "    topics_dict[\"Text\"].append(str_text)\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "print(\"DONE stage2! Retrieving the data\")\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ict_new = pd.DataFrame(topics_dict)\n",
    "ict_all = ict_old.append(ict_new, sort=False, ignore_index=True)\n",
    "\n",
    "ict_all.columns = ict_all.columns.str.strip()\n",
    "ict_all.columns.tolist()\n",
    "\n",
    "ict_all = ict_all.drop_duplicates(['Link'])\n",
    "ict_all.to_csv(\"ict-vacatures-text-all.csv\", encoding = \"utf-8\", index= False)\n",
    "\n",
    "print(len(ict_new), len(ict_old), len(ict_all), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
